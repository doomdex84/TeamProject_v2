# analysis.py
from __future__ import annotations
import os
from dataclasses import dataclass
from typing import Dict, Any, Optional
from pathlib import Path

import numpy as np
import librosa
import requests

from dotenv import load_dotenv
from openai import OpenAI

# ---------------------------
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ & OpenAI ì¤€ë¹„
# ---------------------------
BASE_DIR = Path(__file__).resolve().parent
# âœ… .envê°€ ì—†ì–´ë„ .env.sampleì„ ì½ë„ë¡ ë³´ì¥
for name in (".env", ".env.local", ".env.sample"):
    load_dotenv(BASE_DIR / name, override=False)

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… í‚¤ê°€ ì—†ìœ¼ë©´ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë§Œë“¤ì§€ ì•ŠìŒ (import ì‹œì  í¬ë˜ì‹œ ë°©ì§€)
if not OPENAI_API_KEY:
    print("[analysis.py] WARNING: OPENAI_API_KEY is not set. Using fallback without LLM.")
    client: Optional[OpenAI] = None
else:
    client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… LLM ìƒíƒœ(í”„ë¡ íŠ¸ì—ì„œ í™•ì¸ìš©)
_last_llm_status: Dict[str, Any] = {
    "used": False,
    "ok": False,
    "error": None,
    "model": OPENAI_MODEL,
    "has_api_key": bool(OPENAI_API_KEY),
}

# ---------------------------
# ìœ í‹¸
# ---------------------------
def _safe_float(x, ndigits: int = 5) -> Optional[float]:
    try:
        return round(float(x), ndigits)
    except Exception:
        return None

def _nan_robust(values: np.ndarray, fn, default=None):
    try:
        v = fn(values[~np.isnan(values)])
        if np.isnan(v):
            return default
        return v
    except Exception:
        return default

@dataclass
class VoiceFeatures:
    duration_sec: Optional[float]
    f0_med: Optional[float]
    f0_range: Optional[float]
    energy_mean: Optional[float]
    zcr_mean: Optional[float]
    sc_mean: Optional[float]
    tempo_bpm_like: Optional[float]
    is_silent: bool

    def to_dict(self) -> Dict[str, Any]:
        return {
            "duration_sec": self.duration_sec,
            "f0_med": self.f0_med,
            "f0_range": self.f0_range,
            "energy_mean": self.energy_mean,
            "zcr_mean": self.zcr_mean,
            "sc_mean": self.sc_mean,
            "tempo_bpm_like": self.tempo_bpm_like,
            "is_silent": self.is_silent,
        }

# ---------------------------
# íŠ¹ì§• ì¶”ì¶œ
# ---------------------------
def _extract_features(file_path: str, target_sr: int = 16000) -> VoiceFeatures:
    """
    íŒŒì¼ì—ì„œ ìŒì„± íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ë‚´ë¶€ ê³„ì‚°ìš© â€” ì¶œë ¥ì— ìˆ«ìë¥¼ ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
    """
    y, sr = librosa.load(file_path, sr=target_sr, mono=True)
    duration = _safe_float(librosa.get_duration(y=y, sr=sr), 6)

    # ë¬´ì„± ì—¬ë¶€(ì•ˆì „ì¥ì¹˜)
    rms = librosa.feature.rms(y=y).flatten()
    is_silent = bool(np.mean(rms) < 1e-3)

    # F0 ì¶”ì •(YIN)
    try:
        f0 = librosa.yin(y, fmin=50, fmax=1100, sr=sr)
    except Exception:
        f0 = np.array([np.nan])

    f0_med = _safe_float(_nan_robust(f0, np.nanmedian, default=np.nan), 2)
    f0_p95 = _nan_robust(f0, lambda a: np.nanpercentile(a, 95), default=np.nan)
    f0_p05 = _nan_robust(f0, lambda a: np.nanpercentile(a, 5), default=np.nan)
    f0_range = _safe_float((f0_p95 - f0_p05) if (f0_p95 is not None and f0_p05 is not None) else np.nan, 2)

    # ì—ë„ˆì§€(RMS í‰ê· )
    energy_mean = _safe_float(float(np.mean(rms)) if len(rms) else np.nan, 6)

    # ZCR í‰ê· 
    zcr = librosa.feature.zero_crossing_rate(y=y).flatten()
    zcr_mean = _safe_float(float(np.mean(zcr)) if len(zcr) else np.nan, 6)

    # ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ í‰ê· 
    sc = librosa.feature.spectral_centroid(y=y, sr=sr).flatten()
    sc_mean = _safe_float(float(np.mean(sc)) if len(sc) else np.nan, 2)

    # í…œí¬ ê·¼ì‚¬(bpm)
    try:
        tempo = librosa.beat.tempo(y=y, sr=sr)
        tempo_bpm_like = _safe_float(float(tempo[0]) if tempo.size else np.nan, 2)
    except Exception:
        tempo_bpm_like = None

    return VoiceFeatures(
        duration_sec=duration,
        f0_med=f0_med,
        f0_range=f0_range,
        energy_mean=energy_mean,
        zcr_mean=zcr_mean,
        sc_mean=sc_mean,
        tempo_bpm_like=tempo_bpm_like,
        is_silent=is_silent
    )

# ---------------------------
# ì„±ë³„/ë‚˜ì´ëŒ€ ì¶”ì • (ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±)
# ---------------------------
def _gender_kor_from_features(feat: VoiceFeatures) -> str:
    f0 = feat.f0_med or 0
    if f0 >= 180:
        return "ì—¬"
    if f0 <= 140:
        return "ë‚¨"
    return "ë‚¨"

def _age_phrase_from_features(feat: VoiceFeatures) -> str:
    f0 = feat.f0_med or 0
    sc = feat.sc_mean or 0
    score = 0
    if f0 >= 250: score += 2
    elif f0 <= 120: score -= 2
    if sc >= 2800: score += 1
    elif sc <= 1600: score -= 1
    if score >= 2: return "10ëŒ€~20ëŒ€ ì´ˆë°˜"
    if score >= 0: return "20ëŒ€"
    if score >= -1: return "30ëŒ€"
    return "40ëŒ€ ì´ìƒ"

# ---------------------------
# ìˆ«ì â†’ ì‰¬ìš´ ë§ ë‹¨ì„œë¡œ ë³€í™˜ (ì¶œë ¥ì—ëŠ” ìˆ«ì/ì „ë¬¸ìš©ì–´ ì ˆëŒ€ ê¸ˆì§€)
# ---------------------------
def _describe_bins(features: VoiceFeatures) -> Dict[str, str]:
    """
    ë¶„ì„ ìˆ«ìë¥¼ 'ì‰¬ìš´ ë§' ë‹¨ì„œë¡œë§Œ ë³€í™˜í•´ì„œ LLMì— ë„˜ê¸´ë‹¤.
    ì ˆëŒ€ ìˆ«ì(Hz/BPM/dB ë“±)ë‚˜ ì „ë¬¸ ìš©ì–´(í”¼ì¹˜/ìŠ¤í™íŠ¸ëŸ´/ZCR/RMS ë“±)ë¥¼ ì“°ì§€ ì•ŠëŠ”ë‹¤.
    """
    f = features.to_dict()
    f0_med = f.get("f0_med") or 0.0
    f0_range = f.get("f0_range") or 0.0
    energy = f.get("energy_mean") or 0.0
    tempo  = f.get("tempo_bpm_like") or 0.0
    sc     = f.get("sc_mean") or 0.0
    zcr    = f.get("zcr_mean") or 0.0

    # í†¤(ëŒ€ëµì )
    if f0_med <= 140: tone = "ì¤‘ì €ìŒ"
    elif f0_med >= 200: tone = "ë°ì€ ê³ ìŒ"
    else: tone = "ì¤‘ê°„ ìŒì—­"

    # ìŒë†’ì´ ë³€í™”(í­)
    if f0_range >= 180: variety = "ë„“ìŒ"
    elif f0_range <= 60: variety = "ì¢ìŒ"
    else: variety = "ë³´í†µ"

    # í˜/ì„¸ê¸° ëŠë‚Œ
    if energy is None:
        power = "ë³´í†µ"
    elif energy >= 0.04: power = "íƒ„íƒ„í•¨"
    elif energy <= 0.02: power = "ì•½í•¨"
    else: power = "ë³´í†µ"

    # ì†ë„/ë¦¬ë“¬
    if tempo is None: pace = "ì ë‹¹"
    elif tempo >= 120: pace = "ë¹ ë¥¸ í¸"
    elif tempo <= 90: pace = "ëŠë¦° í¸"
    else: pace = "ì ë‹¹"

    # ë°ê¸°(ìŒìƒ‰ì˜ ë”°ëœ»/ë°ìŒ ëŠë‚Œ)
    if sc >= 2500: brightness = "ë°ì€ í¸"
    elif sc <= 1600: brightness = "ë”°ëœ»í•œ í¸"
    else: brightness = "ì¤‘ê°„"

    # ëª…ë£Œë„(ê±°ì¹ /í˜¸í¡ ì„ì„ ì •ë„ â†’ ë§¤ìš° ëŸ¬í”„)
    if zcr >= 0.12: clarity = "ì¡°ê¸ˆ ê±°ì¹œ ëŠë‚Œ"
    elif zcr <= 0.06: clarity = "ë˜ë ·í•œ í¸"
    else: clarity = "ê´œì°®ì€ í¸"

    return {
        "tone": tone,
        "variety": variety,
        "power": power,
        "pace": pace,
        "brightness": brightness,
        "clarity": clarity,
    }

# ---------------------------
# LLM ì„¤ëª… ìƒì„± (ë™ë¬¼ì˜ ìˆ² ë¬¸êµ¬ ì œê±°)
# ---------------------------
def _llm_describe_voice(features: VoiceFeatures) -> str:
    # âœ… ì „ì—­ ìƒíƒœ ë¨¼ì € ì„ ì–¸
    global client, OPENAI_API_KEY, _last_llm_status

    # ìƒíƒœ ì´ˆê¸°í™”
    _last_llm_status = {
        "used": False,
        "ok": False,
        "error": None,
        "model": OPENAI_MODEL,
        "has_api_key": bool(OPENAI_API_KEY),
    }

    # ğŸ”§ ëŸ°íƒ€ì„ ì¬ì´ˆê¸°í™”: .env/.env.local/.env.sample ì¬ì‹œë„
    if client is None and not OPENAI_API_KEY:
        for name in (".env", ".env.local", ".env.sample"):
            load_dotenv(BASE_DIR / name, override=True)
        OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
        _last_llm_status["has_api_key"] = bool(OPENAI_API_KEY)
        if OPENAI_API_KEY:
            try:
                client = OpenAI(api_key=OPENAI_API_KEY)
            except Exception as e:
                _last_llm_status.update({"error": f"client_init_failed: {e}"})

    # ë¬´ì„±/ì§§ì€ ì…ë ¥
    if features.is_silent or (features.duration_sec or 0) < 0.6:
        return (
            "ì†Œë¦¬ê°€ ê±°ì˜ ì—†ì–´ì„œ ë­ë¼ ë§í•˜ê¸°ê°€ ì–´ë µë„¤. ì£¼ë³€ ì†ŒìŒì„ ì¡°ê¸ˆ ì¤„ì´ê³ , "
            "ì¡°ê¸ˆë§Œ ë” ê¸¸ê²Œ ë§í•´ì£¼ë©´ ëŠë‚Œì„ ë” ì˜ ì¡ì•„ë³¼ê²Œ."
        )

    # ì„±ë³„/ë‚˜ì´ëŒ€/í†¤
    gender = _gender_kor_from_features(features)
    gender_word = "ì—¬ì" if gender == "ì—¬" else "ë‚¨ì"
    age_phrase = _age_phrase_from_features(features)
    bins = _describe_bins(features)

    # í‚¤ ì—†ê±°ë‚˜ client ì‹¤íŒ¨ â†’ í´ë°± (ACNH ë¬¸ì¥ ì—†ì´)
    if client is None:
        return (
            f"{age_phrase} {gender_word} ëª©ì†Œë¦¬ ê°™ê³ , {bins['tone']}ì´ë¼ ì•ˆì •ì ìœ¼ë¡œ ë“¤ë ¤. "
            "ìŒë†’ì´ ë³€í™”ê°€ ë„“ì–´ì„œ ë§ì— ìƒë™ê°ì´ ëŠê»´ì§€ê³ , ë¦¬ë“¬ì€ ì ë‹¹í•´ì„œ íë¦„ì´ ë§¤ë„ëŸ¬ì›Œ. "
            "ê°€ë” í˜ì´ ì‚´ì§ ë¹ ì ¸ ë³´ì¼ ë•Œê°€ ìˆì§€ë§Œ ì „ì²´ì ìœ¼ë¡œëŠ” ë˜ë ·í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œ."
        )

    system_msg = (
        "ë„ˆëŠ” 'ìŒì„± í‰ê°€ ì „ë¬¸ê°€'ì§€ë§Œ ë§íˆ¬ëŠ” ì¹œêµ¬ì²˜ëŸ¼ í¸í•œ ë°˜ë§ë¡œ í•´.\n"
        "ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™:\n"
        "1) ì¶œë ¥ì€ 3~5ë¬¸ì¥ìœ¼ë¡œ.\n"
        "2) ì¥ì /ë‹¨ì  ê· í˜• ìˆê²Œ, ì¡°ì–¸Â·ì§€ì‹œëŠ” ê¸ˆì§€.\n"
        "3) ìˆ«ìÂ·ë‹¨ìœ„Â·ì „ë¬¸ ìš©ì–´ ì ˆëŒ€ ê¸ˆì§€.\n"
        "4) ì²« ë¬¸ì¥ì€ ì„±ë³„/ë‚˜ì´ëŒ€ ì¶”ì¸¡ìœ¼ë¡œ ì‹œì‘.\n"
        "5) ë™ë¬¼ì˜ ìˆ², ìºë¦­í„°í™” ë“±ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆ. ì˜¤ì§ ëª©ì†Œë¦¬ ëŠë‚Œë§Œ ë§í•´.\n"
    )

    user_msg = f"""
- ì„±ë³„ íŒíŠ¸: {gender_word}
- ë‚˜ì´ëŒ€ íŒíŠ¸: {age_phrase}
- í†¤: {bins['tone']}
- ìŒë†’ì´ ë³€í™”: {bins['variety']}
- í˜: {bins['power']}
- ì†ë„: {bins['pace']}
- ë°ê¸°: {bins['brightness']}
- ëª…ë£Œë„: {bins['clarity']}
"""

    try:
        _last_llm_status["used"] = True
        res = client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.9,
            max_tokens=240,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
        )
        text = (res.choices[0].message.content or "").strip()
        _last_llm_status.update({"ok": True})
        return text
    except Exception as e:
        _last_llm_status.update({"ok": False, "error": f"{type(e).__name__}: {e}"})
        print(f"[analysis.py] LLM call failed: {type(e).__name__}: {e}")
        return (
            f"{age_phrase} {gender_word} ëª©ì†Œë¦¬ ê°™ê³ , {bins['tone']}ì´ë¼ ì•ˆì •ì ìœ¼ë¡œ ë“¤ë ¤. "
            "ìŒë†’ì´ ë³€í™”ê°€ ë„“ì–´ì„œ ë§ì— ìƒë™ê°ì´ ëŠê»´ì§€ê³ , ë¦¬ë“¬ì€ ì ë‹¹í•´ì„œ íë¦„ì´ ë§¤ë„ëŸ¬ì›Œ. "
            "ê°€ë” í˜ì´ ì‚´ì§ ë¹ ì ¸ ë³´ì¼ ë•Œê°€ ìˆì§€ë§Œ ì „ì²´ì ìœ¼ë¡œëŠ” ë˜ë ·í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œ."
        )

# ---------------------------
# ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸(ì‚¬ëŒ ì´ë¯¸ì§€, ë„¤ê°€ ì¤€ ë¬¸êµ¬ ê·¸ëŒ€ë¡œ)
# ---------------------------
def _build_dynamic_visual_fields(features: VoiceFeatures) -> Dict[str, Any]:
    """
    ëª©ì†Œë¦¬ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ìºë¦­í„° ì†ì„±ì„ ë™ì ìœ¼ë¡œ ë°˜ì˜í•œ en_prompt ìƒì„±
    """
    bins = _describe_bins(features)
    
    # ê¸°ë³¸ ìºë¦­í„° í”„ë¡¬í”„íŠ¸ (ê³ ì • ìš”ì†Œ)
    base_prompt = (
        "high quality, stylized semi-realistic character art, "
        "soft watercolor tones, painterly texture, "
        "storybook character design, warm and charming mood, "
        "simple plain background"
    )

    dynamic_attrs = ""
    if client:
        # LLMì—ê²Œ ëª©ì†Œë¦¬ ê¸°ë°˜ ì†ì„± ì¶”ì²œ ìš”ì²­
        try:
            user_msg = f"""
            ì´ ëª©ì†Œë¦¬ë¥¼ ê°€ì§„ ì‚¬ëŒì„ ê·¸ë¦´ ë•Œ ì–´ìš¸ë¦¬ëŠ” ìºë¦­í„° ì†ì„±ì„ ì˜ì–´ë¡œ ì¶”ì²œí•´ì¤˜.
            - í†¤: {bins['tone']}
            - ìŒë†’ì´ ë³€í™”: {bins['variety']}
            - í˜/ì„¸ê¸°: {bins['power']}
            - ì†ë„/ë¦¬ë“¬: {bins['pace']}
            - ë°ê¸°: {bins['brightness']}
            - ëª…ë£Œë„: {bins['clarity']}
            ì†ì„±ì—ëŠ” ë¨¸ë¦¬ ìƒ‰, ëˆˆ ìƒ‰, ë¨¸ë¦¬ìŠ¤íƒ€ì¼, ì˜· ìŠ¤íƒ€ì¼, í‘œì • ë“±ì„ í¬í•¨í•˜ê³  1ë¬¸ì¥ìœ¼ë¡œ.
            """
            res = client.chat.completions.create(
                model=OPENAI_MODEL,
                temperature=0.7,
                max_tokens=100,
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ìºë¦­í„° ì•„íŠ¸ ì „ë¬¸ê°€ì•¼. ì˜ì–´ë¡œ 1ë¬¸ì¥ë§Œ ì¶”ì²œí•´."},
                    {"role": "user", "content": user_msg},
                ],
            )
            dynamic_attrs = res.choices[0].message.content.strip()
        except Exception as e:
            print(f"[analysis.py] Dynamic attribute generation failed: {e}")

    # ìµœì¢… en_prompt
    en_prompt = f"{base_prompt}, {dynamic_attrs}" if dynamic_attrs else base_prompt

    return {
        "en_prompt": en_prompt,
        "negative": "text, watermark, logo, extra limbs, distorted anatomy, 3d render",
        "style_tags": ["semi-realistic", "watercolor", "storybook"],
        "palette": ["#F2E5F2", "#FDE2E4", "#E2F0FE", "#FFF4CC"],
        "seed": 3501657520,
    }




# ---------------------------
# (ì„ íƒ) ì•„ë°”íƒ€ íŒíŠ¸ â€” en_promptì—ëŠ” ë” ì´ìƒ ë¶™ì´ì§€ ì•ŠìŒ
# ---------------------------
def _avatar_suggestions(features: VoiceFeatures) -> Dict[str, Any]:
    f = features.to_dict()
    f0_med = f.get("f0_med") or 0.0
    energy = f.get("energy_mean") or 0.0
    tempo  = f.get("tempo_bpm_like") or 0.0

    gender_guess = "female" if f0_med >= 180 else "male"
    vibe = "lively" if tempo and tempo >= 110 else "calm"
    hint = f"{gender_guess} portrait, {vibe} mood"
    if energy and energy >= 0.04:
        hint += ", confident posture"
    return {"avatar_hint_text": hint}

# ---------------------------
# WebUI ì´ë¯¸ì§€ ìƒì„±
# ---------------------------
def generate_image(prompt: str, negative_prompt: str = "", width: int = 512, height: int = 512, steps: int = 20, seed: int = None):
    payload = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "width": width,
        "height": height,
        "steps": steps,
        "seed": seed or -1,
        "sampler_name": "Euler a",
        "cfg_scale": 7.0,
    }
    WEBUI_URL = "http://127.0.0.1:7860"
    resp = requests.post(f"{WEBUI_URL}/sdapi/v1/txt2img", json=payload)
    resp.raise_for_status()
    data = resp.json()
    return data["images"][0]  # Base64

# ---------------------------
# ë©”ì¸ í•¨ìˆ˜
# ---------------------------
def analyze_voice(file_path: str, generate_img: bool = False) -> Dict[str, Any]:
    """
    ìŒì„± íŒŒì¼ì„ ë¶„ì„í•˜ê³ :
    - íŠ¹ì§• ì¶”ì¶œ
    - LLM ì„¤ëª… ìƒì„±
    - ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸/ì•„ë°”íƒ€ íŒíŠ¸
    ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    """
    # 1ï¸âƒ£ íŠ¹ì§• ì¶”ì¶œ
    features = _extract_features(file_path)

    # 2ï¸âƒ£ LLM ì„¤ëª… ìƒì„±
    description_ko = _llm_describe_voice(features)

    # 3ï¸âƒ£ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸/ì•„ë°”íƒ€ íŒíŠ¸
    visual = _build_dynamic_visual_fields(features)
    avatar = _avatar_suggestions(features)
    en_prompt = visual["en_prompt"]

    # 4ï¸âƒ£ WebUI ì´ë¯¸ì§€ ìƒì„± (ì„ íƒ)
    img_base64 = None
    if generate_img:
        try:
            img_base64 = generate_image(
                prompt=en_prompt,
                negative_prompt=visual.get("negative", ""),
                width=512,
                height=512,
                steps=25,
                seed=visual.get("seed")
            )
        except Exception as e:
            print(f"[analysis.py] Image generation failed: {e}")

    # 5ï¸âƒ£ ê²°ê³¼ ë°˜í™˜
    return {
        "features": features.to_dict(),
        "description_ko": description_ko,   # âœ… LLM ê²°ê³¼
        "en_prompt": en_prompt,
        "avatar": avatar,
        "image_base64": img_base64,
        "llm_status": _last_llm_status,     # âœ… í”„ë¡ íŠ¸ì—ì„œ ìƒíƒœ í™•ì¸ìš©
        **visual,                           # âš  style_tags, palette ë“±ë„ í¬í•¨
    }



